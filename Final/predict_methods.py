# -*- coding: utf-8 -*-
"""predict_methods_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dHcTLzk9puJV6baAYh2j_I5AJuNvNFpe
"""

'''This script predicts temperature from three different machine learning models.
    The models are hosted on amazon s3 bucket
    Before starting, you should:
    1)If not done, upload your dataset to your s3 bucket
    2) Set the s3 bucket credentials (access key id and secret access key).
        *If you don't have these credentials, please follow the first part of this tutorial to get them https://realpython.com/python-boto3-aws-s3/
    3)Change the s3_bucket parameter to your bucket name
    4) Set your dataset name as hosted oin your bucket (change the input_name parameter)
'''
#Set the s3 credentials
aws_key_id='', #Change here, set your s3 access key id
aws_secret_key='', #Change here, set your s3 secret access key

#Set the bucket and input file name
s3_bucket = '' #Change here, set your bucket name
input_name = '' #Change here. Set your csv input file name that will be predicted (file needs to be hosted on s3 bucket)
output_name = '' #Change here. Set your csv output file for results after predicting

#Don't change, these are the models link parameters asked by the assignment.
model1 = 'https://log8415-tp2-ml.s3.amazonaws.com/model1.sav' #Model 1: SVM. Don't change
model2 = 'https://log8415-tp2-ml.s3.amazonaws.com/model2.sav' #Model 2: RandomForestRegressor. Don't change
model3 = 'https://log8415-tp2-ml.s3.amazonaws.com/model3.sav' #Model 3: DecisionTreeRegressor. Don't change

'''-------------------------------------------------------------------------'''
import pip
print('------------------Installing required libraries------------------------')
def install(package_name):
  try:
      pipcode = pip.main(['install', package_name])
      if pipcode != 0:
          print("Unable to install " + package_name + " ; pipcode %d" % pipcode)
  except:
      print(package_name + " is already installed")
pkgs = ['wget', 'boto3', 'pandas', 'scikit-learn', 'pickle']
for pkg in pkgs:
  install(pkg)

import boto3
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import r2_score
import pickle
import sys
import wget

'''-------------------------------------------------------------------------'''
print('------------------------Starting the script----------------------------')
#Initialising the connection to the s3 client.
try:
  print('Initialising the s3 bucket...', end ="   "),
  s3 = boto3.client(
      's3',
      aws_access_key_id=''.join(aws_key_id), #your s3 access key id
      aws_secret_access_key=''.join(aws_secret_key), #your s3 secret access key
  )
  print('DONE')
except Exception as e:
  print('Error! Did you set the s3 credentials correctly?')
  print(e)
  sys.exit('Please open the script and set the credentials. Exiting...')

#Reading input file
print("Reading input file...", end ="   "),
try:
  s3.download_file(s3_bucket, input_name,"data.csv")
  data = pd.read_csv('data.csv')
  print('DONE')
except:
  sys.exit('Error! Please specify your s3 bucket name (s3_bucket) and/or your input file name (input_name). Exiting... %tb')

#Starting data preprocessing
print('Starting Data preprocessing...')
print('   +drop all rows with null values...', end ="   ")
data = data.dropna()
print('DONE')

print('   +Encoding String values into numerical values (wd and station)', end ="   "),
labelencoder = LabelEncoder()
data['wd'] = labelencoder.fit_transform(data['wd'])
data['station'] = labelencoder.fit_transform(data['station'])
print('DONE')

print('   +Applying feature scalling for data normalization...', end ="   ")
##Each column of norm_mean_dict is the mean of that column from the dataset used for training
norm_mean_dict = {'CO': 1217.5056897538639,
                  'DEWP': 2.399352604464814,
                  'NO2': 50.12580481224956,
                  'O3': 57.51304219175718,
                  'PM10': 103.88501030337723,
                  'PM2.5': 78.9816471093303,
                  'PRES': 1010.743455396767,
                  'SO2': 15.492473405838535,
                  'hour': 11.579378935317687,
                  'month': 6.472349742415569,
                  'wd': 6.720778477389811,
                  'year': 2014.7166571265025}
#Each column of norm_stdev_dict is the standard deviation of that column from the dataset used for training
norm_stdev_dict = {'CO': 1150.1701750127093,
                   'DEWP': 13.805683774980336,
                   'NO2': 34.969247499380515,
                   'O3': 56.676463334296535,
                   'PM10': 90.84093498213319,
                   'PM2.5': 79.71220652930842,
                   'PRES': 10.440023888928256,
                   'SO2': 21.055878728752777,
                   'hour': 6.934095370818124,
                   'month': 3.449073760899836,
                   'wd': 4.523986069418892,
                   'year': 1.1599380213839083}

columns = ["year", "month", "hour", "PM2.5", "PM10", "SO2", "NO2", "CO", "O3", "PRES", "DEWP", "wd"]
for column in columns:
    data[column] = (data[column] - norm_mean_dict[column]) / norm_stdev_dict[column] + 3
print('DONE')

print('   +Removing unecessary columns No, day, RAIN, WSPM and station (correlation < 1%)...', end ="   "),
try:
  X = data[["year", "month", "hour", "PM2.5", "PM10", "SO2", "NO2", "CO", "O3", "PRES", "DEWP", "wd"]]
  y = data["TEMP"]
except:
  sys.exit('Error! The dataset should have the same columns as those given during the assignment. Exiting...')
print('DONE')

#Start the prediction

#Model3
print('Predicting with third model DecisionTreeRegressor......')
print('   +Downloadind the model from s3 bucket....', end='   ')
wget.download(model3, 'model3.sav')
with open('model3.sav', 'rb') as f:
    pkl_model3 = pickle.load(f)
print('DONE')
    # Calculate the accuracy score and predict target values with model3
print('   +Predicting the new values, please wait....', end='   ')
y_pred_model3 = pkl_model3.predict(X)
score3 = r2_score(y, y_pred_model3)
print('DONE')
print('   +\33[32m' + 'Accuracy score of model 3 : {0:.2f} %'.format(100 * score3) + '\33[0m')

#Model2
print('Predicting with second model RandomForestRegressor......')
print('   +Downloadind the model from s3 bucket....', end='   ')
wget.download(model2, 'model2.sav')
with open('model2.sav', 'rb') as f:
    pkl_model2 = pickle.load(f)
print('DONE')
    # Calculate the accuracy score and predict target values
print('   +Predicting the new values, please wait....', end='   ')
y_pred_model2 = pkl_model2.predict(X)
score2 = r2_score(y, y_pred_model2)
print('DONE')
print('   +\33[32m' + 'Accuracy score of model 2 : {0:.2f} %'.format(100 * score2) + '\33[0m')

#Model1
print('Predicting with first model SVM.SVR...')
print('   +Downloadind the model from s3 bucket....', end='   ')
wget.download(model1, 'model1.sav')
with open('model1.sav', 'rb') as f:
    pkl_model1 = pickle.load(f)
print('DONE')
    # Calculate the accuracy score and predict target values
print('   +Predicting the new values, please wait....', end='   ')
y_pred_model1 = pkl_model1.predict(X)
score1 = r2_score(y, y_pred_model1)
print('DONE')
print('   +\33[32m' + 'Accuracy score of model 1 : {0:.2f} %'.format(100 * score1) + '\33[0m')

#Saving
print('Saving the predicted results to output file...', end='   ')
df_res = pd.DataFrame({'Actual': y, 'model1_pred': y_pred_model1, 'model2_pred': y_pred_model2, 'model3_pred': y_pred_model3})
group_names = ['verycold', 'cold', 'moderate', 'hot', 'veryhot']
bins = [-30, 0, 10, 20, 30, 50]
df_res["Actual"] = pd.cut(df_res["Actual"], bins = bins, include_lowest=True, labels = group_names)
df_res["model1_pred"] = pd.cut(df_res["model1_pred"], bins = bins, include_lowest=True, labels = group_names)
df_res["model2_pred"] = pd.cut(df_res["model2_pred"], bins = bins, include_lowest=True, labels = group_names)
df_res["model3_pred"] = pd.cut(df_res["model3_pred"], bins = bins, include_lowest=True, labels = group_names)

df_res.to_csv (output_name, index = False, header=True)
print('DONE')

#Uploading to S3
print('Uploading result to s3 bucket...', end='   ')
with open(output_name, "rb") as f:
  s3.upload_fileobj(f, s3_bucket, output_name,
                    ExtraArgs={'ACL': 'public-read'}
                    )
print('DONE')
print('------------------Program completed successfully! showing predicted results--------------------')
print('Description of the columns of the output file:')
print('   +Actual: True temperature from the dataset')
print('   +model1_pred: Predicted temperature using model 1')
print('   +model2_pred: Predicted temperature using model 2')
print('   +model3_pred: Predicted temperature using model 3')
print("You can access the result file on s3 with this link https://{0}.s3.amazonaws.com/{1} or you can open the file {1} in your current directory".format(s3_bucket, output_name))
df_res
